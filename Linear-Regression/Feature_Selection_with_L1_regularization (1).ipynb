{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxzR9Eli9Z0S"
      },
      "source": [
        "# Feature selection with L1 Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK2qVQww9rSg"
      },
      "source": [
        "* Consider a supervised learning setting where you have data $\\mathcal{D} = \\{(X, Y)\\}$, where $X \\in \\mathbb{R}^{NXD}$. $N$  - num of datapoints and $D$ - num of features. Many of the features may be irrelevant for the task at hand.\n",
        "* Typically when there are more features than samples, ie. $D >> N$, a simple linear regression model is prone to overfit.\n",
        "* As we have seen before, regularization is one of the tools in our toolbox to combat overfitting. Here we will see how they do that and how $L1$ regularization can be used for feature selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHl0lb8A_MWd"
      },
      "source": [
        "## Tasks\n",
        "\n",
        "1.  Load the data \"cancer_reg.csv\" data from [here](https://data.world/nrippner/ols-regression-challenge). A slightly modified copy will be shared with you on workplace.\n",
        "2. Preprocess your data particularly making sure there are no nan values.\n",
        "3. Split the data into 20\\% train and 80\\% test sets.\n",
        "4. Define your linear regression model with and without regularization ($L1$ and $L2$)\n",
        "5. Train and test your linear regression models (without regularization) till it overfits the data. Plot the learning curves for training and testing (You may ignore loss values for the first 10 epochs)\n",
        "6. Train and test again with regularization and visually inspect the effects.\n",
        "7. Plot the weights for each model (using matplotlib stem plot). Do you see any difference between them?\n",
        "8. In the case of the $L1$ regularization, set a small threshold and remove all features of the data corresponding to the weights with values below this threshold.\n",
        "9. Train a new model with remaining features (with or without regularization)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmMz0oqMU5PJ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}