{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYW2YAMZOX4-"
      },
      "source": [
        "# Reinforcement Learning in Finite MDPs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNlEnGsYOX5A"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "from gym import utils\n",
        "sys.path.insert(0, './mvarl_hands_on/utils')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7uh6UnZsWdh",
        "outputId": "bd90c3d4-a812-4acd-91e0-b898bfb42529"
      },
      "source": [
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1\n",
        "!cd mvarl_hands_on && git pull"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgWMSWNOvr73"
      },
      "source": [
        "# Exercice 1: Understanding Value-Function and Q-function\n",
        "\n",
        "In this exercice, we are going to learn:\n",
        "\n",
        "*   What is a MDP?\n",
        "*   How to evaluate the quality of a policy in a MDP (Value-iteration and Policy-Iteration)\n",
        "*   How to move from V-function to Q-function\n",
        "*   How to move from Q-function to greedy-policy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puk7xD1EBDEu"
      },
      "source": [
        "### Step 1: Dealing with MDP and RL environment\n",
        "\n",
        "Here, we are going to use the famous cleaning robot MDP.\n",
        "http://www.incompleteideas.net/sutton/book/first/3/node7.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7DVrtCu1xJ6",
        "cellView": "form"
      },
      "source": [
        "# @title Robot MDP implementation (You do not need to read this code)\n",
        "\n",
        "class RobotEnv:\n",
        "    \"\"\"\n",
        "    Enviroment with 2 states and 3 actions\n",
        "    Args:\n",
        "        gamma (float): discount factor\n",
        "        seed    (int): Random number generator seed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gamma=0.5, seed=42):\n",
        "        # Set seed\n",
        "        self._RS = np.random.RandomState(seed)\n",
        "\n",
        "        # Transition probabilities\n",
        "        # shape (Ns, Na, Ns)\n",
        "        # P[s, a, s'] = Prob(S_{t+1}=s'| S_t = s, A_t = a)\n",
        "\n",
        "        self._Ns = 2\n",
        "        self._Na = 3\n",
        "        self._gamma = gamma\n",
        "        \n",
        "        # Note we add a recharge option in state A with a negative reward (to have a well defined matrix-transition)\n",
        "        self._P = np.array([[[1, 0], [3/4, 1/4], [1, 0]], [[0,1],[1,0], [1,0]]])\n",
        "        self._R = np.array([[0,1,-0.5], [0, -1, 0]])\n",
        "\n",
        "        self._state_decoder  = {0: \"High\", 1: \"Low\"}\n",
        "        self._action_decoder = {0: \"WAIT\", 1: \"SEARCH\", 2: \"RECHARGE\"}\n",
        "        \n",
        "        # Initialize base class\n",
        "        self._states = np.arange(self.Ns).tolist()\n",
        "        self._action_sets = [np.arange(self.Na).tolist()]*self.Ns\n",
        "\n",
        "    ### Utils\n",
        "    def render_state(self, state):\n",
        "      return self._state_decoder[state]\n",
        "\n",
        "    def render_action(self, action):\n",
        "      return self._action_decoder[action] \n",
        "\n",
        "    def render_policy(self, policy):\n",
        "      if len(np.array(policy).shape) > 1:\n",
        "        policy = densify_policy(policy)\n",
        "\n",
        "      txt = \"\"\n",
        "      for i, a in enumerate(policy):\n",
        "        txt += \"In state {} perform {}\\n\".format(self._state_decoder[i], self._action_decoder[a])\n",
        "      return txt[:-1]\n",
        "\n",
        "    ### MDP properties\n",
        "    @property\n",
        "    def states(self):\n",
        "      return self._states \n",
        "\n",
        "    @property\n",
        "    def actions(self):\n",
        "      return self._action_sets \n",
        "\n",
        "    @property\n",
        "    def transition_matrix(self):\n",
        "      return self._P\n",
        "\n",
        "    @property\n",
        "    def reward_matrix(self):\n",
        "      return self._R\n",
        "    \n",
        "    @property\n",
        "    def gamma(self):\n",
        "      return self._gamma\n",
        "\n",
        "    @property\n",
        "    def Ns(self):\n",
        "      return self._Ns\n",
        "\n",
        "    @property\n",
        "    def Na(self):\n",
        "      return self._Na\n",
        "\n",
        "    ### Interact with environment\n",
        "    def reward_func(self, state, action, *_):\n",
        "      return self._R[state, action]\n",
        "\n",
        "    def sample_transition(self, s, a):\n",
        "        prob = self._P[s,a,:]\n",
        "        next_s = self._RS.choice(self.states, p = prob)\n",
        "        return next_s\n",
        "\n",
        "    def reset(self, new_initial_state=0):\n",
        "        assert new_initial_state < self.Ns\n",
        "        self.state = new_initial_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = self.sample_transition(self.state, action)\n",
        "        reward = self.reward_func(self.state, action, next_state)\n",
        "        done = False\n",
        "        info = {\"str\" : \"In {} do {} arrive at {} get {}\".format(\n",
        "            self._state_decoder[state],\n",
        "            self._action_decoder[action],\n",
        "            self._state_decoder[next_state],\n",
        "            reward )}\n",
        "        self.state = next_state\n",
        "\n",
        "        observation = next_state\n",
        "        return observation, reward, done, info\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYqt4muO1yij"
      },
      "source": [
        "# create the environment\n",
        "env = RobotEnv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbC3m3wO9Imp"
      },
      "source": [
        "A MDP have is a tuple with ($S$, $A$, $R$, $P$, $\\gamma$)\n",
        "*   $S$ is the state space\n",
        "*   $A$ is the action space\n",
        "*   $R$ is the reward function\n",
        "*   $P$ is the transition kernel. If I am in state $s$, and take the action $a$, what is the probability of moving to state $s'$\n",
        "*   $\\gamma$ is the discount factor, i.e., how far in the future you are looking for rewards (gamma=0 means, you just take immediate reward, gammma=0.9 you look at reward around 10 steps away)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FA5vUBd7X9q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5af04fdf-f0e6-4b7f-d0b6-b38a8ce15030"
      },
      "source": [
        "# Display the MDP classic information\n",
        "\n",
        "print(\"Number of states: \", env.Ns, [env.render_state(s) for s in range(env.Ns)])\n",
        "print(\"Number of actions: \", env.Na, [env.render_action(a) for a in range(env.Na)])\n",
        "print(\"\")\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of available actions per state:\", env.actions)\n",
        "print(\"\")\n",
        "print(\"P has shape: \", env.transition_matrix.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"R has shape: \", env.reward_matrix.shape)  \n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of states:  2 ['High', 'Low']\n",
            "Number of actions:  3 ['WAIT', 'SEARCH', 'RECHARGE']\n",
            "\n",
            "Set of states: [0, 1]\n",
            "Set of available actions per state: [[0, 1, 2], [0, 1, 2]]\n",
            "\n",
            "P has shape:  (2, 3, 2)\n",
            "R has shape:  (2, 3)\n",
            "discount factor:  0.5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R_hP0wjEFT4"
      },
      "source": [
        "A MDP is a mathematical representation of an environement. Here, we are going to interact with this environment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6S_ja2FDXHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73461568-7721-4585-9416-8dc164fc1525"
      },
      "source": [
        "state=0\n",
        "action=1\n",
        "print(f\"State {state}: battery is\", env.render_state(state))\n",
        "print(f\"Action {action}: robot performs\", env.render_action(action))\n",
        "print(f\"Reward at state={state} and action={action}) is\", env.reward_func(state,action))\n",
        "\n",
        "next_state = env.sample_transition(state,action)\n",
        "print(\"Next (stochastic) state is\", env.render_state(next_state))  # you can keep running this cell colab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State 0: battery is High\n",
            "Action 1: robot performs SEARCH\n",
            "Reward at state=0 and action=1) is 1.0\n",
            "Next (stochastic) state is High\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm8TPqLe-LP_"
      },
      "source": [
        "Finally, we here define a helper to step in the environment. Let's try to follow a random policy by picking a random action $a$ at everytime step $t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OC-zSnd9ExV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62b17809-7ec6-4970-d04e-ef52ce003f1b"
      },
      "source": [
        "# Interact with environment\n",
        "\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state, \":\", env.render_state(state))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "# Interacting with the environment by tacking random action\n",
        "print(\"s:   a:   s':   r:\")\n",
        "for time in range(4):\n",
        "    action = np.random.randint(env.Na) # Pick random action\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(f\"{state}    {action}    {next_state}    {reward} \\t --> \" + info[\"str\"] if \"str\" in info else \"\") \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial state:  0 : High\n",
            "\n",
            "s:   a:   s':   r:\n",
            "0    2    0    -0.5 \t --> In High do RECHARGE arrive at High get -0.5\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z7SImS4Fpta"
      },
      "source": [
        "It is also possible to define a deterministic policy which associate an action $a$ for every state $s$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZGmYI3OCKXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daddf685-ecd7-49f3-d751-fac4a91d191c"
      },
      "source": [
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "print(env.render_policy(policy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random policy =  [0 0]\n",
            "In state High perform WAIT\n",
            "In state Low perform WAIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL1883coHXIF"
      },
      "source": [
        "## Question 1: \n",
        "Hand-craft the optimal policy (High=search, Low=recharge), display it, and interact with the environment for 5 steps\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApI4TrT-HWkC"
      },
      "source": [
        "my_policy = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMk_C05rGGVs"
      },
      "source": [
        "From now on, you should have understood how to interact with an environment, and retrieve the MDP information.\n",
        "\n",
        "If you do not, ping your TA ;) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z7d9_NsHQXM"
      },
      "source": [
        "# Step 2: Evaluating a policy\n",
        "In this subsection, we aim at estimating the quality of a predefined policy, i.e, how much reward can I expect if I follow any policy (even if this policy is not optimal)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YM7lLbRA68A"
      },
      "source": [
        "\n",
        "## Useful functions\n",
        "In the following exercice, there is a constant back-and-forth between dense and sparse representation of policy. For instance, taking the action $a=2$ may be encoded by:\n",
        "\n",
        "*   Dense Represention: a=2\n",
        "*   Sparse Represention: a=[0, 0, 1]\n",
        "\n",
        "To help you to move from dense and sparse, policy, we provide you those two functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sekFIFmm6V71"
      },
      "source": [
        "def sparsify_policy(policy, Na):\n",
        "  ### Turn a dense policy into a sparse one.\n",
        "  #  Ex: [0, 1], Na=2  -> [[1, 0, 0], [0, 1, 0]]\n",
        "  ###\n",
        "\n",
        "  Ns = len(policy)\n",
        "  sparse_policy = np.zeros(shape=(Ns, Na))\n",
        "  for i, a in enumerate(policy):\n",
        "    sparse_policy[i,a]=1\n",
        "  return sparse_policy\n",
        "\n",
        "def densify_policy(policy):\n",
        "  ### Turn a sparse determinist policy into a dense one.\n",
        "  #  Ex: [[1, 0, 0], [0, 1, 0]] -> [0, 1]\n",
        "  ###\n",
        "  return np.array(policy).argmax(axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDT3iHZ8JNLO"
      },
      "source": [
        "## Question 2: Policy Evaluation\n",
        "Let's now start serious things! Have a look to slide 37, compute the dynamics and rewards given the policy, and solve the linear system on V to evaluate the policy.\n",
        "\n",
        "First, compute the policy normalized transition/rewards\n",
        "$$P^{\\pi}(s, s') = \\sum_a{\\pi(s|a)P(s,a,s')}$$\n",
        "$$R^{\\pi}(s) = \\sum_a{\\pi(s|a)R(s,a)}$$\n",
        "\n",
        "Then, compute the value function\n",
        "$$V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}$$\n",
        "\n",
        "which can be turned into\n",
        "$$V^{\\pi} = (I - \\gamma P^{\\pi})^{-1} R^{\\pi}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOVL2QrdJMrI"
      },
      "source": [
        "# Policy evaluation (exact)\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.transition_matrix\n",
        "R = env.reward_matrix\n",
        "gamma = env.gamma\n",
        "\n",
        "# Policy to evaluate\n",
        "# State A: Search\n",
        "# State B: Wait\n",
        "dense_policy = np.array([1, 0])  # sub-optinal policy... on purpose!\n",
        "sparse_policy = sparsify_policy(dense_policy, Na=env.Na)\n",
        "\n",
        "print(\"## pi:\")\n",
        "print(env.render_policy(dense_policy))\n",
        "\n",
        "# Compute the dynamics given the policy\n",
        "\n",
        "# Naive implementation\n",
        "Ppi = np.zeros([2, 2])  # Hint: Ppi is of shape [2,2] -> it was normalized by the action\n",
        "for state in range(env.Ns):\n",
        "  for next_state in range(env.Ns):\n",
        "    Ppi[state, next_state] = np.sum(sparse_policy[state] * P[state, :, next_state])\n",
        "\n",
        "Rpi = np.zeros([2])\n",
        "for state in range(env.Ns):\n",
        "  Rpi[state] = np.sum(sparse_policy[state] * R[state, :])\n",
        "\n",
        "# Matrix form\n",
        "Ppi = np.sum(P * sparse_policy[..., None], axis=1)\n",
        "Rpi = np.sum(R * sparse_policy, axis=1)\n",
        "\n",
        "\n",
        "# Evaluate the policy\n",
        "Vpi = np.linalg.inv(np.identity(env.Ns) - gamma * Ppi).dot(Rpi)\n",
        "print(\"## Vpi: \", Vpi)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnODmP6TR1-_"
      },
      "source": [
        "## Question 2 (Bonus): Implement the recursive implementation of value evaluation\n",
        "\n",
        "You want to evaluate the policy by iterating the fixed point equation on V, starting from a randomly initialized V function.\n",
        "\n",
        "In other words:\n",
        "\n",
        "To compute the value function\n",
        "$$V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}$$\n",
        "\n",
        "you can use the contractive property of Bellman \n",
        "$$V^{\\pi}_{k+1} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}_k$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hHiqwbKSCCc"
      },
      "source": [
        "# Compute Value Iteration\n",
        "\n",
        "# Policy evaluation (recursive)\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.P\n",
        "R = env.R\n",
        "gamma = env.gamma\n",
        "\n",
        "\n",
        "# Policy to evaluate\n",
        "dense_policy = np.array([1, 0])\n",
        "sparse_policy = sparsify_policy(dense_policy, Na=env.Na)\n",
        "print(sparse_policy)\n",
        "print(env.render_policy(sparse_policy))\n",
        "\n",
        "\n",
        "# Stopping criterion\n",
        "# Feel free to use the any valid stopping criterion (max_iteration or inf_norm)\n",
        "epsilon = 1e-3\n",
        "\n",
        "\n",
        "# Estimate V (please print v at each iteration k)\n",
        "v = ...\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dcvnxbuOa1z"
      },
      "source": [
        "## Question 3: Turning V-function into Q-function\n",
        "What is the Q-function for this value function ?\n",
        "\n",
        "$$Q^{\\pi}(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s, a, s')V^{\\pi}(s')$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rLK8scKO0CV"
      },
      "source": [
        "# Compute the Q values\n",
        "Qpi = np.zeros([env.Ns, env.Na])\n",
        "# Hint: look at the previous code ;)\n",
        "for state in range(env.Ns):\n",
        "  for action in range(env.Na):\n",
        "    Qpi[state, action] = R[state, action] + gamma * np.sum(P[state, action, :]*Vpi)\n",
        "\n",
        "print(\"## Qpi:\")\n",
        "print(Qpi)\n",
        "\n",
        "\n",
        "Qpi = R   + gamma  *    np.sum(P  *  Vpi[None, None, :], axis=-1)\n",
        "Qpi = R   + gamma  *    P  @ Vpi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAzH417WO7Nu"
      },
      "source": [
        "The Q-function is a useful way to evaluate the policy. Yet, it can also be used to improve the policy! To do so, you can create a new policy by taking the argmax of the Q-function (improvment step). \n",
        "\n",
        "What is the next policy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvglCCIjPUaf"
      },
      "source": [
        "# Compute the Q values\n",
        "Qpi = R + gamma * P @ Vpi\n",
        "print(\"## Qpi:\")\n",
        "print(Qpi)\n",
        "\n",
        "# What is the next policy if we perform one step of policy improvment ?\n",
        "new_policy = Qpi.argmax(axis=-1)\n",
        "print(\"## new pi:\")\n",
        "print(new_policy)\n",
        "print(env.render_policy(new_policy))\n",
        "\n",
        "# Compute the value of the NEW policy\n",
        "new_sparse_policy = sparsify_policy(new_policy, Na=env.Na)\n",
        "\n",
        "new_Ppi = np.sum(P * new_sparse_policy[..., None], axis=1)\n",
        "new_Rpi = np.sum(R * new_sparse_policy, axis=1)\n",
        "new_Vpi = np.linalg.inv(np.identity(env.Ns) - gamma * new_Ppi) @ new_Rpi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu0Iu7xMP9Wp"
      },
      "source": [
        "## Policy Improvement in MDP\n",
        "\n",
        "In slide 50-51, we introduce two algorithms to obtain the optimal policy from a sub-optimal one (by using different shade of policy improvment shapes). \n",
        "\n",
        "*   Value iteration: Efficient algorithms with exact values but it requires inverting a matrix\n",
        "*   Policy iteration: Slighly less efficient algorithm with potential approximation, but no need to invert matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNY2iNLpVYCG",
        "cellView": "form"
      },
      "source": [
        "# @title New Environemnt (You do not need to read this code)\n",
        "\n",
        "\n",
        "class GridWorldWithPits:\n",
        "    def __init__(self, grid, txt_map, gamma=0.99, proba_succ=0.95, uniform_trans_proba=0.001, normalize_reward=False):\n",
        "        self.desc = np.asarray(txt_map, dtype='c')\n",
        "        self.grid = grid\n",
        "        self.txt_map = txt_map\n",
        "\n",
        "        self.action_names = np.array(['right', 'down', 'left', 'up'])\n",
        "\n",
        "        self.n_rows, self.n_cols = len(self.grid), max(map(len, self.grid))\n",
        "\n",
        "        # Create a map to translate coordinates [r,c] to scalar index\n",
        "        # (i.e., state) and vice-versa\n",
        "        self.normalize_reward = normalize_reward\n",
        "\n",
        "\n",
        "        self.initial_state = None\n",
        "        self.coord2state = np.empty_like(self.grid, dtype=np.int)\n",
        "        self.nb_states = 0\n",
        "        self.state2coord = []\n",
        "        for i in range(self.n_rows):\n",
        "            for j in range(len(self.grid[i])):\n",
        "                if self.grid[i][j] != 'w':\n",
        "                    if self.grid[i][j] == 's':\n",
        "                        self.initial_state = self.nb_states\n",
        "                    self.coord2state[i, j] = self.nb_states\n",
        "                    self.nb_states += 1\n",
        "                    self.state2coord.append([i, j])\n",
        "                else:\n",
        "                    self.coord2state[i, j] = -1\n",
        "\n",
        "        self.P = None\n",
        "        self.R = None\n",
        "        self.proba_succ = proba_succ\n",
        "        self.uniform_trans_proba = uniform_trans_proba\n",
        "\n",
        "        # compute the actions available in each state\n",
        "        self.state_actions = [range(len(self.action_names)) for _ in range(self.nb_states)]#self.compute_available_actions()\n",
        "        self.matrix_representation()\n",
        "        self.lastaction = None\n",
        "        self.current_step = 0\n",
        "  \n",
        "        self._actions = self.state_actions\n",
        "        self._gamma = gamma\n",
        "\n",
        "\n",
        "    def matrix_representation(self):\n",
        "        if self.P is None:\n",
        "            nstates = self.nb_states\n",
        "            nactions = max(map(len, self.state_actions))\n",
        "            self.P = np.inf * np.ones((nstates, nactions, nstates))\n",
        "            self.R = np.inf * np.ones((nstates, nactions))\n",
        "            for s in range(nstates):\n",
        "                r, c = self.state2coord[s]\n",
        "                for a_idx, action in enumerate(range(len(self.action_names))):\n",
        "                    self.P[s, a_idx].fill(0.)\n",
        "                    if self.grid[r][c] == 'g':\n",
        "                        self.P[s, a_idx, self.initial_state] = 1.\n",
        "                        self.R[s, a_idx] = 10.\n",
        "                    else:\n",
        "                        ns_succ, ns_fail = np.inf, np.inf\n",
        "                        if action == 0:\n",
        "                            ns_succ = self.coord2state[r, min(self.n_cols - 1, c + 1)]\n",
        "                            ns_fail = [self.coord2state[r, max(0, c - 1)],\n",
        "                            self.coord2state[min(self.n_rows - 1, r + 1), c],\n",
        "                            self.coord2state[max(0, r - 1), c]\n",
        "                            ]\n",
        "\n",
        "                        elif action == 1:\n",
        "                            ns_succ = self.coord2state[min(self.n_rows - 1, r + 1), c]\n",
        "                            ns_fail = [self.coord2state[max(0, r - 1), c],\n",
        "                            self.coord2state[r, max(0, c - 1)],\n",
        "                            self.coord2state[r, min(self.n_cols - 1, c + 1)]\n",
        "                            ]\n",
        "                        elif action == 2:\n",
        "                            ns_succ = self.coord2state[r, max(0, c - 1)]\n",
        "                            ns_fail = [self.coord2state[r, min(self.n_cols - 1, c + 1)],\n",
        "                            self.coord2state[max(0, r - 1), c],\n",
        "                            self.coord2state[min(self.n_rows - 1, r + 1), c]\n",
        "                            ]\n",
        "                        elif action == 3:\n",
        "                            ns_succ = self.coord2state[max(0, r - 1), c]\n",
        "                            ns_fail = [self.coord2state[min(self.n_rows - 1, r + 1), c],\n",
        "                            self.coord2state[r, min(self.n_cols - 1, c + 1)],\n",
        "                            self.coord2state[r, max(0, c - 1)]\n",
        "                            ]\n",
        "\n",
        "                        L = []\n",
        "                        for el in ns_fail:\n",
        "                            x, y = self.state2coord[el]\n",
        "                            if self.grid[x][y] == 'w':\n",
        "                                L.append(s)\n",
        "                            else:\n",
        "                                L.append(el)\n",
        "\n",
        "                        self.P[s, a_idx, ns_succ] = self.proba_succ\n",
        "                        for el in L:\n",
        "                            self.P[s, a_idx, el] += (1. - self.proba_succ)/len(ns_fail)\n",
        "                        # self.P[s, a_idx] = self.P[s, a_idx] + self.uniform_trans_proba / nstates\n",
        "                        # self.P[s, a_idx] = self.P[s, a_idx] / np.sum(self.P[s, a_idx])\n",
        "\n",
        "                        assert np.isclose(self.P[s, a_idx].sum(), 1)\n",
        "\n",
        "                        if self.grid[r][c] == 'x':\n",
        "                            self.R[s, a_idx] = -20\n",
        "                        else:\n",
        "                            self.R[s, a_idx] = -2\n",
        "\n",
        "            if self.normalize_reward:\n",
        "                minr = np.min(self.R)\n",
        "                maxr = np.max(self.R[np.isfinite(self.R)])\n",
        "                self.R = (self.R - minr) / (maxr - minr)\n",
        "\n",
        "            self.d0 = np.zeros((nstates,))\n",
        "            self.d0[self.initial_state] = 1.\n",
        "\n",
        "    def compute_available_actions(self):\n",
        "        # define available actions in each state\n",
        "        # actions are indexed by: 0=right, 1=down, 2=left, 3=up\n",
        "        state_actions = []\n",
        "        for i in range(self.n_rows):\n",
        "            for j in range(self.n_cols):\n",
        "                if self.grid[i][j] == 'g':\n",
        "                    state_actions.append([0])\n",
        "                elif self.grid[i][j] != 'w':\n",
        "                    actions = [0, 1, 2, 3]\n",
        "                    if i == 0:\n",
        "                        actions.remove(3)\n",
        "                    if j == self.n_cols - 1:\n",
        "                        actions.remove(0)\n",
        "                    if i == self.n_rows - 1:\n",
        "                        actions.remove(1)\n",
        "                    if j == 0:\n",
        "                        actions.remove(2)\n",
        "\n",
        "                    for a in copy.copy(actions):\n",
        "                        r, c = i, j\n",
        "                        if a == 0:\n",
        "                            c = min(self.n_cols - 1, c + 1)\n",
        "                        elif a == 1:\n",
        "                            r = min(self.n_rows - 1, r + 1)\n",
        "                        elif a == 2:\n",
        "                            c = max(0, c - 1)\n",
        "                        else:\n",
        "                            r = max(0, r - 1)\n",
        "                        if self.grid[r][c] == 'w':\n",
        "                            actions.remove(a)\n",
        "\n",
        "                    state_actions.append(actions)\n",
        "        return state_actions\n",
        "\n",
        "    def description(self):\n",
        "        desc = {\n",
        "            'name': type(self).__name__\n",
        "        }\n",
        "        return desc\n",
        "\n",
        "    def reward_func(self, state, action, next_state):\n",
        "        return self.R[state, action]\n",
        "\n",
        "    def reset(self, s=None):\n",
        "        self.lastaction = None\n",
        "        if s is None:\n",
        "            self.state = self.initial_state\n",
        "        else:\n",
        "            self.state = s\n",
        "        self.current_step = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        try:\n",
        "            action_index = self.state_actions[self.state].index(action)\n",
        "        except:\n",
        "            raise ValueError(\"Action {} cannot be executed in this state {}\".format(action, self.state))\n",
        "\n",
        "        p = self.P[self.state, action_index]\n",
        "        next_state = np.random.choice(self.nb_states, 1, p=p).item()\n",
        "\n",
        "        reward = self.R[self.state, action_index]\n",
        "\n",
        "        self.lastaction = action\n",
        "\n",
        "        r, c = self.state2coord[self.state]\n",
        "        done = self.grid[r][c] == 'g'\n",
        "        self.current_step +=1\n",
        "        self.state = next_state\n",
        "\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "    def render_state(self, state):\n",
        "\n",
        "        out = self.desc.copy().tolist()\n",
        "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
        "        r, c = self.state2coord[state]\n",
        "\n",
        "        def ul(x):\n",
        "            return \"_\" if x == \" \" else x\n",
        "\n",
        "        if self.grid[r][c] == 'x':\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(out[1 + r][2 * c + 1], 'red', highlight=True)\n",
        "        elif self.grid[r][c] == 'g':  # passenger in taxi\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(ul(out[1 + r][2 * c + 1]), 'green', highlight=True)\n",
        "        else:\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(ul(out[1 + r][2 * c + 1]), 'yellow', highlight=True)\n",
        "\n",
        "        return \"\\n\".join([\"\".join(row) for row in out]) + \"\\n\"\n",
        "\n",
        "    def render_action(self, action):\n",
        "      return self.action_names[action] \n",
        "\n",
        "    def render_policy(self, pol):\n",
        "        out = self.desc.copy().tolist()\n",
        "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
        "        r, c = self.state2coord[self.state]\n",
        "\n",
        "        for s in range(self.Ns):\n",
        "            r, c = self.state2coord[s]\n",
        "            action = pol[s]\n",
        "            # 'right', 'down', 'left', 'up'\n",
        "            if action == 0:\n",
        "                out[1 + r][2 * c + 1] = '>'\n",
        "            elif action == 1:\n",
        "                out[1 + r][2 * c + 1] = 'v'\n",
        "            elif action == 2:\n",
        "                out[1 + r][2 * c + 1] = '<'\n",
        "            elif action == 3:\n",
        "                out[1 + r][2 * c + 1] = '^'\n",
        "            else:\n",
        "                raise ValueError()\n",
        "\n",
        "        return \"\\n\".join([\"\".join(row) for row in out]) + \"\\n\"\n",
        "\n",
        "    def copy(self):\n",
        "        new_env = GridWorldWithPits(grid=self.grid, txt_map=self.txt_map,\n",
        "                                    proba_succ=self.proba_succ, uniform_trans_proba=self.uniform_trans_proba)\n",
        "        return new_env\n",
        "\n",
        "    def sample_transition(self, s, a):\n",
        "        try:\n",
        "            p = self.P[s, a]\n",
        "        except:\n",
        "            raise ValueError(\"Action {} cannot be executed in this state {}\".format(action, self.state))\n",
        "        next_state = np.random.choice(self.nb_states, 1, p=p).item()\n",
        "\n",
        "    ### MDP properties\n",
        "    @property\n",
        "    def states(self):\n",
        "      return np.zeros([self.n_cols, self.n_rows]) \n",
        "\n",
        "    @property\n",
        "    def actions(self):\n",
        "      return range(4)\n",
        "\n",
        "    @property\n",
        "    def transition_matrix(self):\n",
        "      return self.P\n",
        "\n",
        "    @property\n",
        "    def reward_matrix(self):\n",
        "      return self.R\n",
        "    \n",
        "    @property\n",
        "    def gamma(self):\n",
        "      return self._gamma\n",
        "\n",
        "    @property\n",
        "    def Ns(self):\n",
        "      return self.n_cols * self.n_rows\n",
        "\n",
        "    @property\n",
        "    def Na(self):\n",
        "      return 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbuuOZj7VOgs"
      },
      "source": [
        "# New Maze environment\n",
        "# s: start\n",
        "# g: goal\n",
        "# x: negative reward state\n",
        "\n",
        "grid1 = [\n",
        "    ['', '', '', 'g'],\n",
        "    ['', 'x', '', ''],\n",
        "    ['s', '', '', '']\n",
        "]\n",
        "grid1_MAP = [\n",
        "    \"+-------+\",\n",
        "    \"| : : :G|\",\n",
        "    \"| :x: : |\",\n",
        "    \"|S: : : |\",\n",
        "    \"+-------+\",\n",
        "]\n",
        "\n",
        "env = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv2pOuPuFajI"
      },
      "source": [
        "# Useful attributes\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of actions:\", env.actions)\n",
        "print(\"Number of states: \", env.Ns)\n",
        "print(\"Number of actions: \", env.Na)\n",
        "print(\"P has shape: \", env.transition_matrix.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"R has shape: \", env.reward_matrix.shape)  \n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")\n",
        "\n",
        "# Usefult methods\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state)\n",
        "print(\"reward at (s=0, a=1,s'=1): \", env.reward_func(0,1,1))\n",
        "print(\"\")\n",
        "\n",
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "\n",
        "# Interacting with the environment\n",
        "print(\"(s, a, s', r):\")\n",
        "for time in range(4):\n",
        "    action = policy[state]\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(state, action, next_state, reward) \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKhH4UyoFVp1"
      },
      "source": [
        "# Define a fixed random policy\n",
        "dense_policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(env.render_policy(dense_policy))\n",
        "\n",
        "# Start a new episode\n",
        "state = env.reset()\n",
        "print(\"start:\")\n",
        "print(env.render_state(state))\n",
        "\n",
        "# Follow the policy\n",
        "for i in range(3):\n",
        "    action = dense_policy[state]\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    print(action, \":\", env.render_action(action))\n",
        "    print(env.render_state(state))\n",
        "    if done:\n",
        "      break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwIpq7j5SqwU"
      },
      "source": [
        "# useful function\n",
        "def plot_v(all_v, v_star):\n",
        "  \n",
        "  all_v = np.array(all_v)\n",
        "  v_star = np.array(v_star)\n",
        "\n",
        "  # Compute inf norm\n",
        "  diff = np.absolute(all_v - v_star).max(axis=1)\n",
        "\n",
        "  plt.plot(diff)\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Error')\n",
        "  plt.title(\"||V* - V||_inf\")\n",
        "\n",
        "# useful function\n",
        "def plot_infnorm(lst, star, name=\"V\"):\n",
        "  \n",
        "  lst = np.array(lst)\n",
        "  star = np.array(star)\n",
        "\n",
        "  # Compute inf norm\n",
        "  diff = np.absolute(lst - star).max(axis=1)\n",
        "  plt.figure()\n",
        "  plt.plot(diff)\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Error')\n",
        "  plt.title(\"||{} - {}*||_inf\".format(name, name))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E6VKYj2xUWx"
      },
      "source": [
        "## Question 3: Implement Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL_Lr6oOGZ3W"
      },
      "source": [
        "def sparsify_policy(policy, Na):\n",
        "  ### Turn a dense policy into a sparse one.\n",
        "  #  Ex: [0, 1], Na=2  -> [[1, 0, 0], [0, 1, 0]]\n",
        "  ###\n",
        "\n",
        "  Ns = len(policy)\n",
        "  sparse_policy = np.zeros(shape=(Ns, Na))\n",
        "  for i, a in enumerate(policy):\n",
        "    sparse_policy[i,a]=1\n",
        "  return sparse_policy\n",
        "\n",
        "def densify_policy(policy):\n",
        "  ### Turn a sparse determinist policy into a dense one.\n",
        "  #  Ex: [[1, 0, 0], [0, 1, 0]] -> [0, 1]\n",
        "  ###\n",
        "  return np.array(policy).argmax(axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXpp55q8QquM"
      },
      "source": [
        "# Compute Policy Iteration\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.transition_matrix\n",
        "R = env.reward_matrix\n",
        "gamma = env.gamma \n",
        "\n",
        "# Prepare v, and storage\n",
        "v = np.zeros(env.Ns)\n",
        "v_all = []\n",
        "\n",
        "dense_policy = np.zeros(shape=(env.Ns,), dtype=np.int32) + 2\n",
        "dense_policy_all = []\n",
        "\n",
        "# iterate over the value\n",
        "while True:\n",
        "  \n",
        "  print(env.render_policy(dense_policy))\n",
        "\n",
        "  # Sparsify policy to perform matrix operation\n",
        "  sparse_policy = sparsify_policy(dense_policy, Na=env.Na)\n",
        "\n",
        "  # Compute v (and intermediate values)\n",
        "  ...\n",
        "\n",
        "  # Policy improvement step\n",
        "  ...\n",
        "\n",
        "  # store v_k\n",
        "  v_all.append(v)\n",
        "  dense_policy_all.append(new_dense_policy)\n",
        "\n",
        "  # stopping criterion \n",
        "  if ...: \n",
        "    break\n",
        "\n",
        "  dense_policy = new_dense_policy\n",
        "\n",
        "\n",
        "# Display final results\n",
        "print(\"  ###  Final results  ###\")\n",
        "print(env.render_policy(new_dense_policy))\n",
        "plot_infnorm(dense_policy_all, new_dense_policy, name=\"Pi\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMUC1QlHStsu"
      },
      "source": [
        "## Question 3 (Bonus): Implement Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSKtZCytSstg"
      },
      "source": [
        "# compute optimal policy\n",
        "policy = ...\n",
        "v_star = ...\n",
        "\n",
        "print(env.render_policy(policy))\n",
        "plot_v(v_all, v_star)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHrlauOwOX5L"
      },
      "source": [
        "# Exercice 2: Q learning\n",
        "Q learning is a **model-free** algorithm for estimating the optimal Q-function **online**. \n",
        "\n",
        "Being **model-free** means that it doesn't assume knowledge of $P$ and $r$, only that we can interact with the environment.\n",
        "\n",
        "Being **online** means that we update, and hopefully improve our\n",
        "policy with each step that we are making in the environment.\n",
        "\n",
        "It is an **off-policy** algorithm. This means that the samples we use to update our **learnt** policy are collected with an **acting**  policy that is (potentially) not the **learnt** one, and not the one associated to the estimated Q-function.\n",
        "\n",
        "Q-learning works as follows:\n",
        "- **Initialization**: Initialize a current estimated Q-function $Q$ to $0$. Receive an initial state $s$ from the environment.\n",
        "- **Iterate**:\n",
        "  -  Pick an action according to an $\\varepsilon$-greedy version of the argmax policy on $Q$, i.e. with probability $\\varepsilon$, pick a random uniform action, with probability $1 - \\varepsilon$, pick the argmax of $Q(s, a)$.\n",
        "  - Observe the next state $s'$ and new reward $r$.\n",
        "  - Update $Q$ using the quadruplet $(s, a, r, s')$ with learning rate $\\alpha$\n",
        "  $$Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha (r + \\gamma \\max\\limits_{a'} Q(s', a'))$$\n",
        "\n",
        "1. Implement Q learning with $\\epsilon$-greedy exploration.\n",
        "  - Plot the error in Q-functions over iterations\n",
        "  - Plot the cumulative sum of rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsmojgubLVL9"
      },
      "source": [
        "# main algorithmic loop\n",
        "qvalues = []\n",
        "rewards = []\n",
        "\n",
        "state = env.reset()\n",
        "t = 0\n",
        "max_steps = 1000\n",
        "\n",
        "while t < max_steps:\n",
        "    \n",
        "    # Sample the action\n",
        "    action = ...\n",
        "    \n",
        "    # Sample the environment\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    \n",
        "    # Update q-function\n",
        "    ...\n",
        "\n",
        "    # Store information \n",
        "    rewards.append(reward)\n",
        "    qvalues.append(...)\n",
        "    \n",
        "    state = observation\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "    \n",
        "    # iterate\n",
        "    t = t + 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_VJo-11ubCN"
      },
      "source": [
        "# Evaluation:\n",
        "state = env.reset()\n",
        "env.render()\n",
        "for i in range(5):\n",
        "    action = ...\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuCSKJx_uKHg"
      },
      "source": [
        "As a next step, we are going to encapsulate the Q-learning information into a class. The idea is to structure the algorithm such as an Q-agent is trained. Therefore, the training loop should be unchanged, you only need to code the QAgent class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMBabfSguI7m"
      },
      "source": [
        "# ---------------------------\n",
        "# Q-Agent\n",
        "# ---------------------------\n",
        "class QAgent:\n",
        "    \"\"\"\n",
        "    Q learning with epsilon-greedy exploration\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, learning_rate, epsilon, min_epsilon):\n",
        "      pass\n",
        "    \n",
        "    def sample_action(self, state, greedy=False):\n",
        "      pass\n",
        "    \n",
        "    def update(self, state, action, next_state, reward):\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysm4Cq_pxj5T"
      },
      "source": [
        "# Traning:\n",
        "q_agent = QAgent(env, gamma=env.gamma, learning_rate=1., epsilon=0.5, min_epsilon=0.01)\n",
        "\n",
        "# main algorithmic loop\n",
        "qvalues = []\n",
        "rewards = []\n",
        "\n",
        "state = env.reset()\n",
        "t = 0\n",
        "max_steps = 1000\n",
        "\n",
        "while t < max_steps:\n",
        "    \n",
        "    # Sample the action\n",
        "    action = q_agent.sample(state)\n",
        "    \n",
        "    # Sample the environment\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    \n",
        "    # Update q-function\n",
        "    qvalues = q_agent.update(state, action, next_state, reward)\n",
        "\n",
        "    # Store information \n",
        "    rewards.append(reward)\n",
        "    qvalues.append(qvalues)\n",
        "    \n",
        "    state = observation\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "    \n",
        "    # iterate\n",
        "    t = t + 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59xsIjpIOX5P"
      },
      "source": [
        "## Exercice: SARSA (Homework)\n",
        "# Exercice 2: Sarsa\n",
        "Sarsa is a **model-free** algorithm for estimating the optimal Q-function **online**. \n",
        "\n",
        "Being **model-free** means that it doesn't assume knowledge of $P$ and $r$, only that we can interact with the environment.\n",
        "\n",
        "Being **online** means that we update, and hopefully improve our\n",
        "policy with each step that we are making in the environment.\n",
        "\n",
        "It is an **on-policy** algorithm. This means that the samples we use to update our **learnt** policy are collected with an **acting**  policy that is the **learnt** one, i.e. the one associated to the estimated Q-function.\n",
        "\n",
        "Q-learning works as follows:\n",
        "- **Initialization**: Initialize a current estimated Q-function $Q$ to $0$. Receive an initial state $s$ from the environment. Pick an action $a$ according to a softmax version of $Q$ with temperature $\\tau$, i.e. sample action $a$ with probability \n",
        "  $$\\pi(a | s) = \\frac{\\exp \\frac{Q(s,a)}{\\tau}}{\\sum\\limits_{a'} \\exp \\frac{Q(s,a)}{\\tau}}.$$\n",
        "\n",
        "- **Iterate**: \n",
        "  - Play action $a$.\n",
        "  - Observe the next state $s'$ and new reward $r$.\n",
        "  - Pick an action $a'$ according to a softmax version of $Q$ with temperature $\\tau$, i.e. sample action $a$ with probability \n",
        "  $$\\pi(a' | s') = \\frac{\\exp \\frac{Q(s',a')}{\\tau}}{\\sum\\limits_{a''} \\exp \\frac{Q(s',a'')}{\\tau}}.$$\n",
        "  - Update $Q$ using the quintuplet $(s, a, r, s', a')$ (thus the name SARSA) with learning rate $\\alpha$\n",
        "  $$Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha (r + \\gamma Q(s', a'))$$\n",
        "  - $a \\leftarrow a'$.\n",
        "  - (Optional) Lower the temperature $\\tau$.\n",
        "\n",
        "1. Implement SARSA with softmax (Gibbs) exploration and test the convergence to $Q^\\star$\n",
        "2. Plot the value $\\|V_n - V^\\star\\|_{\\infty}$\n",
        "3. Plot the expected cumulative reward of the algorithms: $t \\mapsto \\sum_{i=1}^t r_i$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gySnHtrsOX5Q"
      },
      "source": [
        "# ---------------------------\n",
        "# SARSA\n",
        "# ---------------------------\n",
        "class SARSA:\n",
        "    \"\"\"\n",
        "    SARSA with deacreasing epsilon for exploration\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, learning_rate, epsilon):\n",
        "      # Start with a random policy\n",
        "      pass\n",
        "    \n",
        "    def sample_action(self, state, greedy=False):\n",
        "      pass\n",
        "        \n",
        "    def update(self, state, action, next_state, next_action, reward):\n",
        "      pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be0d4nOBOX5R"
      },
      "source": [
        "\n",
        "sarsa = SARSA(env, gamma=env.gamma, learning_rate=1., epsilon=1.)\n",
        "\n",
        "\n",
        "# Learn the optimal policy by interacting with the environment\n",
        "...\n",
        "\n",
        "\n",
        "# Plot the value function error \n",
        "...\n",
        "\n",
        "\n",
        "# Plot the expected return \n",
        "...\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}